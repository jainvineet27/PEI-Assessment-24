{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a PEI Assessment Ingestion Transformation Load\n",
    "\n",
    "We have ingested the data, the data found to be in all three different formats Configurations are uploaded to make the parsing and date conversion in Spark version >= 3.0 We have done bunch of transformations : - missing nulls hence done their removal and showed original and new count cleaning of data incuding phone performing cleaning and make it normalized column such as phone Performed join operation However I assume the pyspark queries needs to be translated back to spark sql queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName('pei-assessment')\\\n",
    "    .config('spark.sql.shufflePartitons',3)\\\n",
    "    .enableHiveSupport() \\\n",
    "    .master('local[*]')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://USGURVINEETJA78.us.deloitte.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pei-assessment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x263e5b4a850>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f734e10-d4c0-4a5a-a3b8-49b440b7917b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  installation of packages \n",
    "\n",
    "pip install openpyxl\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "pip install spark-excel\n",
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acce722e-ecde-4046-8e6d-15ed661e9ddb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pandasDF = pd.read_excel(io = 'Customer.xlsx', engine='openpyxl', sheet_name = 'Worksheet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>address</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PW-19240</td>\n",
       "      <td>Pierre Wener</td>\n",
       "      <td>bettysullivan808@gmail.com</td>\n",
       "      <td>421.580.0902x9815</td>\n",
       "      <td>001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80027</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GH-14410</td>\n",
       "      <td>Gary567 Hansen</td>\n",
       "      <td>austindyer948@gmail.com</td>\n",
       "      <td>001-542-415-0246x314</td>\n",
       "      <td>00347 Murphy Unions\\nAshleyton, IA 29814</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>60653</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KL-16555</td>\n",
       "      <td>Kelly Lampkin</td>\n",
       "      <td>clarencehughes280@gmail.com</td>\n",
       "      <td>7185624866</td>\n",
       "      <td>007 Adams Lane Suite 176\\nEast Amyberg, IN 34581</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>80906</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer ID   Customer Name                        email  \\\n",
       "0    PW-19240    Pierre Wener   bettysullivan808@gmail.com   \n",
       "1    GH-14410  Gary567 Hansen      austindyer948@gmail.com   \n",
       "2    KL-16555   Kelly Lampkin  clarencehughes280@gmail.com   \n",
       "\n",
       "                  phone                                            address  \\\n",
       "0     421.580.0902x9815  001 Jones Ridges Suite 338\\nJohnsonfort, FL 95462   \n",
       "1  001-542-415-0246x314           00347 Murphy Unions\\nAshleyton, IA 29814   \n",
       "2            7185624866   007 Adams Lane Suite 176\\nEast Amyberg, IN 34581   \n",
       "\n",
       "       Segment        Country              City     State  Postal Code  \\\n",
       "0     Consumer  United States        Louisville  Colorado        80027   \n",
       "1  Home Office  United States           Chicago  Illinois        60653   \n",
       "2    Corporate  United States  Colorado Springs  Colorado        80906   \n",
       "\n",
       "    Region  \n",
       "0     West  \n",
       "1  Central  \n",
       "2     West  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandasDF=pandasDF.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Customer ID      object\n",
       "Customer Name    object\n",
       "email            object\n",
       "phone            object\n",
       "address          object\n",
       "Segment          object\n",
       "Country          object\n",
       "City             object\n",
       "State            object\n",
       "Postal Code       int64\n",
       "Region           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema= 'customer_id string , customer_name string , email string ,phone string ,address string , segment string,country string ,city string , state string , postal_code string , region string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF.rename(columns={'Customer ID':'customer_id', 'Customer Name':'customer_name', 'Country':'country','City':'city','State':'state','Postal Code':'postal_code','Region':'region'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDF = pandasDF.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = pandasDF.to_csv('customer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df=spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .schema(customer_schema)\\\n",
    "    .load('customer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found discrepancy Checking in converted customer dataframe from pandas df hence checking additional row and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def df_count_match(value):\n",
    "    print(f'checking whether {value} is equals to orignal pandas datafram count {len(pandasDF)} ')\n",
    "    return value > len(pandasDF)\n",
    "\n",
    "# Test class to test the is_positive function\n",
    "class CustomerDataFrameCheck(unittest.TestCase):\n",
    "    # Test case for a positive value\n",
    "    def test_positive_value(self):\n",
    "        expected_result=False\n",
    "        #self.assertTrue(is_positive(10))\n",
    "        actual_result = df_count_match(customer_df.count())\n",
    "        print(f'{actual_result} and {actual_result}')\n",
    "        self.assertEqual(expected_result, actual_result)\n",
    "    def reformattingdataframeandchecking(self):\n",
    "        expected_result=True\n",
    "        #self.assertTrue(is_positive(10))\n",
    "        actual_result = df_count_match(customer_df.count())\n",
    "        print(f'{actual_result} and {actual_result}')\n",
    "        self.assertEqual(expected_result, actual_result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_positive_value (__main__.CustomerDataFrameCheck)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vineetjain8\\AppData\\Local\\Temp\\ipykernel_5624\\3913042169.py\", line 15, in test_positive_value\n",
      "    self.assertEqual(expected_result, actual_result)\n",
      "AssertionError: False != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.112s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking whether 1586 is equals to orignal pandas datafram count 793 \n",
      "True and True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test cases\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(CustomerDataFrameCheck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### found redundant count appeard while comparing with pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df =customer_df.filter(col('customer_id').rlike('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def df_count_match(value):\n",
    "    print(f'checking whether {value} is equals to orignal pandas datafram count {len(pandasDF)} ')\n",
    "    return value == len(pandasDF)\n",
    "\n",
    "# Test class to test the is_positive function\n",
    "class CustomerDataFrameCheck(unittest.TestCase):\n",
    "    # Test case for a positive value\n",
    "    def test_positive_value(self):\n",
    "        expected_result=True\n",
    "        #self.assertTrue(is_positive(10))\n",
    "        actual_result = df_count_match(customer_df.count())\n",
    "        print(f'{actual_result} and {actual_result}')\n",
    "        self.assertEqual(expected_result, actual_result)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.123s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking whether 793 is equals to orignal pandas datafram count 793 \n",
      "True and True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test cases\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(CustomerDataFrameCheck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "609bdd43-b97b-4fd1-9b13-332607cb14fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_df =spark.read.format('json').option('header',True)\\\n",
    "            .option('inferSchema',True)\\\n",
    "            .option('multiline',True)\\\n",
    "    .load('Order.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2e8937-c4c6-4485-8f74-6a75dc471886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_schema = 'product_id string, category string,sub_category string, product_name string,state string, price_per_product float'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edfb48ee-898e-4220-86bb-565c8672f3f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_df = spark.read\\\n",
    "        .format('csv')\\\n",
    "            .option('header',True)\\\n",
    "                    .schema(product_schema)\\\n",
    "                    .load('Product.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a454b2fa-f992-4a79-915e-e646c9f69e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+------------+--------------------+--------+-----------------+\n",
      "|     product_id|  category|sub_category|        product_name|   state|price_per_product|\n",
      "+---------------+----------+------------+--------------------+--------+-----------------+\n",
      "|FUR-CH-10002961| Furniture|      Chairs|Leather Task Chai...|New York|           81.882|\n",
      "|TEC-AC-10004659|Technology| Accessories|Imation Secure+ H...|Oklahoma|            72.99|\n",
      "+---------------+----------+------------+--------------------+--------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e5c71f-fd58-4d8c-968d-4124dda4ea17",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Source DataFrames \n",
    "### customer_df, order_df , product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e297f8a0-f796-466e-ab5d-5ccfbdd58e9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_df = order_df.withColumnRenamed('Customer ID','customer_id')\\\n",
    "    .withColumnRenamed('Product ID','product_id')\\\n",
    "        .withColumnRenamed('Order Date','order_date')\\\n",
    "        .withColumnRenamed('Discount','discount')\\\n",
    "            .withColumnRenamed('Order ID','order_id')\\\n",
    "                .withColumnRenamed('Profit','profit')\\\n",
    "                    .withColumnRenamed('Quantity','quantity')\\\n",
    "                        .withColumnRenamed('Row ID','row_id')\\\n",
    "                            .withColumnRenamed('Ship Date','ship_date')\\\n",
    "                                .withColumnRenamed('Ship Mode','ship_mode')\\\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ce311b-567d-47ab-ae8c-4a156db8ee19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+---------+--------------+\n",
      "|customer_id|discount|order_date|      order_id|  Price|     product_id| profit|quantity|row_id|ship_date|     ship_mode|\n",
      "+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+---------+--------------+\n",
      "|   JK-15370|     0.3| 21/8/2016|CA-2016-122581|573.174|FUR-CH-10002961| 63.686|       7|     1|25/8/2016|Standard Class|\n",
      "|   BD-11320|     0.0| 23/9/2017|CA-2017-117485| 291.96|TEC-AC-10004659|102.186|       4|     2|29/9/2017|Standard Class|\n",
      "+-----------+--------+----------+--------------+-------+---------------+-------+--------+------+---------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1851"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product_df= product_df.isNull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking presence of null in Product Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53bef926-db38-4333-8bed-fa8beb4f0fcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_new_Df = product_df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfda4e23-5cd4-403b-a75c-d3c47ac1400c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe has product_df 1851 rows and  after removing null we have  product_new_Df with 1799  rows \n"
     ]
    }
   ],
   "source": [
    "print(f'Original dataframe has product_df {product_df.count()} rows and  after removing null we have  product_new_Df with {product_new_Df.count()}  rows ') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b48ddb-a767-4b17-9c6e-cb47d79ba092",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Profit rounded to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c29497-abcc-42d9-be4a-580ef186a087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "date_format='yyyy-MM-dd'\n",
    "order_df= order_df.withColumn('profit', round(col('profit'),2))\\\n",
    "        .withColumn('order_date',trim(col('order_date')))\\\n",
    "            .withColumn('ship_date',trim(col('order_date')))\\\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7974eac-94ad-431c-a95b-7ae66eb7068e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+---------+--------------+\n",
      "|customer_id|discount|order_date|      order_id|  Price|     product_id|profit|quantity|row_id|ship_date|     ship_mode|\n",
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+---------+--------------+\n",
      "|   JK-15370|     0.3| 21/8/2016|CA-2016-122581|573.174|FUR-CH-10002961| 63.69|       7|     1|21/8/2016|Standard Class|\n",
      "|   BD-11320|     0.0| 23/9/2017|CA-2017-117485| 291.96|TEC-AC-10004659|102.19|       4|     2|23/9/2017|Standard Class|\n",
      "|   LB-16795|     0.7| 6/10/2016|US-2016-157490|     17|OFF-BI-10002824|-14.92|       4|     3|6/10/2016|   First Class|\n",
      "|   KB-16315|     0.2|  2/7/2015|CA-2015-111703| 15.552|OFF-PA-10003349|  5.64|       3|     4| 2/7/2015|Standard Class|\n",
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+---------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking presence of  characters such as ERRROR,.,( )  in phone columns of customers dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def df_error_match(value):\n",
    "    print(f'checking presence of unknown characters  ')\n",
    "    isErrorWordExists = customer_df.filter(col('phone').rlike('#ERROR!')).count()\n",
    "    return 0 == isErrorWordExists\n",
    "\n",
    "# Test class to test the is_positive function\n",
    "class CustomerDataFrameCheck(unittest.TestCase):\n",
    "    # Test case for a positive value\n",
    "    def test_error_word(self):\n",
    "        expected_result=False\n",
    "        #self.assertTrue(is_positive(10))\n",
    "        actual_result = df_error_match(customer_df.count())\n",
    "        print(f'{actual_result} and {actual_result}')\n",
    "        self.assertEqual(expected_result, actual_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.182s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking presence of unknown characters  \n",
      "False and False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test cases\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(CustomerDataFrameCheck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60a23a4a-cd8f-48d9-bca7-8d888897c23f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Enriching customer name \n",
    "Customer name and country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5d0bbe-23ce-4520-bbdd-c2505d5e5bcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------------------+----------+--------------------+-------+-------+----+-----+-----------+------+\n",
      "|customer_id|      customer_name|               email|     phone|             address|segment|country|city|state|postal_code|region|\n",
      "+-----------+-------------------+--------------------+----------+--------------------+-------+-------+----+-----+-----------+------+\n",
      "|   PW-19240|       Pierre Wener|bettysullivan808@...|4215800902|001 Jones Ridges ...|   null|   null|null| null|       null|  null|\n",
      "|   GH-14410|     Gary567 Hansen|austindyer948@gma...|0015424150| 00347 Murphy Unions|   null|   null|null| null|       null|  null|\n",
      "|   KL-16555|      Kelly Lampkin|clarencehughes280...|7185624866|007 Adams Lane Su...|   null|   null|null| null|       null|  null|\n",
      "|   AH-10075|Ad.       ..am Hart|angelabryant256@g...|2651015569|01454 Christopher...|   null|   null|null| null|       null|  null|\n",
      "|   PF-19165|         Philip Fox|kristinereynolds5...|0014736452|0158 Harris Ways ...|   null|   null|null| null|       null|  null|\n",
      "|   SC-20680|      Steve Carroll|jasoncontreras178...|5636474830| 01630 Tammy Prairie|   null|   null|null| null|       null|  null|\n",
      "|   AB-10105|      Adrian Barton|daviddavis980@gma...|0674358553|  021 Katherine Mall|   null|   null|null| null|       null|  null|\n",
      "|   PT-19090|   Pete@#$ Takahito|mikaylaarnold666@...|7866386820|   0236 Lane Squares|   null|   null|null| null|       null|  null|\n",
      "|   SG-20605|   Speros Goranitis|brianjoyce110@gma...|3528465094|02401 Angela Loop...|   null|   null|null| null|       null|  null|\n",
      "|   MH-17785|        Maya Herman|christinasalas345...|7223765599|      026 Colon Hill|   null|   null|null| null|       null|  null|\n",
      "+-----------+-------------------+--------------------+----------+--------------------+-------+-------+----+-----+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#customer_df_bkp = customer_df\n",
    "customer_df =customer_df.filter(col('phone') !='#ERROR!').filter(lit(length(col('phone'))>lit(5))).withColumn('phone',regexp_replace('phone','x[0-9]+',''))\\\n",
    "        .withColumn('phone',regexp_replace('phone','[\\\\.\\\\-\\\\(\\\\)]',''))\\\n",
    "            .withColumn('phone',substring(col('phone'),1,10))\n",
    "customer_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def df_error_match(value):\n",
    "    print(f'checking presence of unknown characters  ')\n",
    "    isErrorWordExists = customer_df.filter(col('customer_name').rlike('[@.<>#$%^&*]')).count()\n",
    "    return 0 == isErrorWordExists\n",
    "\n",
    "# Test class to test the is_positive function\n",
    "class CustomerNameDataFrameCheck(unittest.TestCase):\n",
    "    # Test case for a positive value\n",
    "    def test_error_word(self):\n",
    "        expected_result=True\n",
    "        #self.assertTrue(is_positive(10))\n",
    "        actual_result = df_error_match(customer_df.count())\n",
    "        print(f'{actual_result} and {actual_result}')\n",
    "        self.assertEqual(expected_result, actual_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_error_word (__main__.CustomerNameDataFrameCheck)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vineetjain8\\AppData\\Local\\Temp\\ipykernel_5624\\2574361274.py\", line 16, in test_error_word\n",
      "    self.assertEqual(expected_result, actual_result)\n",
      "AssertionError: True != False\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.199s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking presence of unknown characters  \n",
      "False and False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test cases\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(CustomerNameDataFrameCheck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0941f305-157f-4553-ad20-6ce98412fa6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import * \n",
    "customer_df = customer_df.withColumn('customer_name',trim('customer_name'))\\\n",
    ".withColumn('customer_name',regexp_replace('customer_name','[@.<>#$%^&*]',''))\\\n",
    "    .withColumn('customer_name',regexp_replace('customer_name','^\\\\s+|\\\\s+$',''))\\\n",
    "         .withColumn('customer_name',regexp_replace('customer_name','\\\\s+',' '))\n",
    "\n",
    "      \n",
    "\n",
    "#print(f'old customer_df count is {customer_df.count()} and new customer df count is {new_customer_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "def df_error_match(value):\n",
    "    print(f'checking presence of unknown characters  ')\n",
    "    isErrorWordExists = customer_df.filter(col('customer_name').rlike('[@.<>#$%^&*]')).count()\n",
    "    return 0 == isErrorWordExists\n",
    "\n",
    "# Test class to test the is_positive function\n",
    "class CustomerNameDataFrameCheck(unittest.TestCase):\n",
    "    # Test case for a positive value\n",
    "    def test_error_word(self):\n",
    "        expected_result=True\n",
    "        #self.assertTrue(is_positive(10))\n",
    "        actual_result = df_error_match(customer_df.count())\n",
    "        print(f'{actual_result} and {actual_result}')\n",
    "        self.assertEqual(expected_result, actual_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.198s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking presence of unknown characters  \n",
      "True and True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the test cases\n",
    "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(CustomerNameDataFrameCheck))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf418c9-4412-41f9-bc6a-51789c87c21d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Result Set from Question 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf260e68-ab51-454e-b5f4-233752e26ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "problem1_1 = order_df\n",
    "problem1_2 = customer_df.select('customer_name','country')\n",
    "problem1_3=product_new_Df.select('product_name','sub_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65746af4-93c4-4182-b06e-ed52414fa14c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74874983-30a7-4928-899f-f937952e11f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_df = order_df.withColumn('order_date', to_date('order_date','dd/MM/yyyy'))\\\n",
    "        .withColumn('ship_date',to_date('ship_date','dd/MM/yyyy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df= order_df.withColumn('profit', col('profit').cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6928bf53-be29-4336-a822-7c64e8d0b4a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+----------+--------------+\n",
      "|customer_id|discount|order_date|      order_id|  Price|     product_id|profit|quantity|row_id| ship_date|     ship_mode|\n",
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+----------+--------------+\n",
      "|   JK-15370|     0.3|2016-08-21|CA-2016-122581|573.174|FUR-CH-10002961| 63.69|       7|     1|2016-08-21|Standard Class|\n",
      "|   BD-11320|     0.0|2017-09-23|CA-2017-117485| 291.96|TEC-AC-10004659|102.19|       4|     2|2017-09-23|Standard Class|\n",
      "|   LB-16795|     0.7|2016-10-06|US-2016-157490|     17|OFF-BI-10002824|-14.92|       4|     3|2016-10-06|   First Class|\n",
      "|   KB-16315|     0.2|2015-07-02|CA-2015-111703| 15.552|OFF-PA-10003349|  5.64|       3|     4|2015-07-02|Standard Class|\n",
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+----------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "015de687-6e44-4b35-ba9e-9eb24dd6efe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create an aggregate table that shows profit by\n",
    "\n",
    "Year\n",
    "\n",
    "Product Category\n",
    "\n",
    "Product Sub Category\n",
    "\n",
    "Customer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614e0b9b-b4f4-42ab-aa49-98b257a2d1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_df = order_df.withColumn('year', year(col('order_date')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+----------+--------------+----+\n",
      "|customer_id|discount|order_date|      order_id|  Price|     product_id|profit|quantity|row_id| ship_date|     ship_mode|year|\n",
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+----------+--------------+----+\n",
      "|   JK-15370|     0.3|2016-08-21|CA-2016-122581|573.174|FUR-CH-10002961| 63.69|       7|     1|2016-08-21|Standard Class|2016|\n",
      "|   BD-11320|     0.0|2017-09-23|CA-2017-117485| 291.96|TEC-AC-10004659|102.19|       4|     2|2017-09-23|Standard Class|2017|\n",
      "|   LB-16795|     0.7|2016-10-06|US-2016-157490|     17|OFF-BI-10002824|-14.92|       4|     3|2016-10-06|   First Class|2016|\n",
      "+-----------+--------+----------+--------------+-------+---------------+------+--------+------+----------+--------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614e0b9b-b4f4-42ab-aa49-98b257a2d1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView('orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92144006-9b54-40a4-91bd-246dd5769b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profit_by_year = order_df.groupBy('year').agg(sum('profit').alias('total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba1fff9-e9bc-4c11-8e39-e1afa13ca411",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df= customer_df.withColumnRenamed('customer_id','customer__id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|customer__id|\n",
      "+------------+\n",
      "|    PW-19240|\n",
      "|    GH-14410|\n",
      "|    KL-16555|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.select('customer__id').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer__id', 'string'),\n",
       " ('customer_name', 'string'),\n",
       " ('email', 'string'),\n",
       " ('phone', 'string'),\n",
       " ('address', 'string'),\n",
       " ('segment', 'string'),\n",
       " ('country', 'string'),\n",
       " ('city', 'string'),\n",
       " ('state', 'string'),\n",
       " ('postal_code', 'string'),\n",
       " ('region', 'string')]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8db5da7-1af9-401d-a1b8-67f1d154fb0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merge_df =order_df.join(customer_df ,order_df.customer_id==customer_df.customer__id , how='inner')\\\n",
    "        .join(product_new_Df, on='product_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802bc07a-ca16-400e-ab49-246da8241dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|       category|cat_wise_sum|\n",
      "+---------------+------------+\n",
      "|Office Supplies|   114968.33|\n",
      "|      Furniture|      3361.5|\n",
      "|     Technology|   104157.16|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_df.groupBy('category').agg(round(sum('profit'),2).alias('cat_wise_sum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527b21b3-605b-4fec-adcc-290d933d1d53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|sub_category|sub_cat_wise_sum|\n",
      "+------------+----------------+\n",
      "|   Envelopes|         5107.39|\n",
      "|         Art|         5180.91|\n",
      "|      Chairs|        21508.91|\n",
      "| Furnishings|          129.33|\n",
      "|    Supplies|        -1525.05|\n",
      "|   Fasteners|          626.88|\n",
      "|     Binders|        33692.77|\n",
      "|   Bookcases|        -2900.87|\n",
      "|      Labels|         4785.94|\n",
      "|       Paper|        26986.03|\n",
      "| Accessories|        40141.17|\n",
      "|     Copiers|        47911.96|\n",
      "|      Phones|        38156.26|\n",
      "|    Machines|       -22052.23|\n",
      "|     Storage|         19891.1|\n",
      "|  Appliances|        20222.36|\n",
      "|      Tables|       -15375.87|\n",
      "+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_df.groupBy('sub_category').agg(round(sum('profit'),2).alias('sub_cat_wise_sum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c415234e-e706-4db2-a7c2-e7237cea4ae8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|customer_id|customerprofit|\n",
      "+-----------+--------------+\n",
      "|   VW-21775|       -882.39|\n",
      "|   RR-19315|        -73.83|\n",
      "|   PB-19210|          21.9|\n",
      "|   EM-13960|         102.3|\n",
      "|   MS-17530|         81.77|\n",
      "|   KH-16630|        734.07|\n",
      "|   BD-11500|       1102.74|\n",
      "|   SW-20275|        332.76|\n",
      "|   AH-10690|       1239.86|\n",
      "|   JF-15415|        624.41|\n",
      "|   PH-18790|         47.89|\n",
      "|   PW-19240|       1233.03|\n",
      "|   IM-15070|        553.89|\n",
      "|   KF-16285|       1664.29|\n",
      "|   NW-18400|       1007.85|\n",
      "|   OT-18730|       -925.56|\n",
      "|   JH-15985|       -644.48|\n",
      "|   KD-16615|        539.72|\n",
      "|   MG-18145|         82.82|\n",
      "|   SM-20320|      -1978.12|\n",
      "+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merge_df.groupBy('customer_id').agg(round(sum('profit'),2).alias('customerprofit')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df.createOrReplaceTempView('orders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL output the following aggregates\n",
    "Profit by Year\n",
    "Profit by Year + Product Category\n",
    "Profit by Customer\n",
    "Profit by Customer + Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select  round(sum(profit),2) as yearly_profit from orders group by  year ').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select round(sum(profit),2) as Total_profit from orders group by year, category').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select round(sum(profit),2)  as customer_profit from orders group by customer_id ').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select round(sum(profit),2) as Totalprofit from orders group by customer_id, year').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b79fbcb-f211-45f4-8dd3-bc02b61a289a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "Only loading customer_df.write\n",
    ".mode('overwrite')\n",
    ".format('parquet')\n",
    ".partitionBy('city')\n",
    ".option('path','product')\n",
    ".save()\n",
    "\n",
    "product.write\n",
    ".format('parquet')\n",
    ".mode('overwrite')\n",
    ".partitionBy('state')\n",
    ".option('path','product')\n",
    ".save()\n",
    "\n",
    "product.write\n",
    ".format('parquet')\n",
    ".mode('overwrite')\n",
    ".partitionBy('order_date')\n",
    ".bucketBy(4,'customer_id')\n",
    ".option('path','product')\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open for suggestions and improvement\n",
    "\n",
    "********************************************* Thanks ********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pei-assessment",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
